{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark --master yarn --conf spark.ui.port=12888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce – The programming model that is used for Distributed computing is known as MapReduce. The MapReduce model involves two stages, Map and Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Map – The mapper processes each line of the input data (it is in the form of a file), and produces key – value pairs.\n",
    "\n",
    "Input data → Mapper → list([key, value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce – The reducer processes the list of key – value pairs (after the Mapper’s function). It outputs a new set of key – value pairs.\n",
    "\n",
    "list([key, value]) → Reducer → list([key, list(values)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web services called sparkContext sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7\"\n",
    "\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init(r'/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf, SparkContext \n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Test\") \n",
    "# setMaster(local) - we are doing tasks on a single machine \n",
    "sc = SparkContext(conf = conf) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method textFile in module pyspark.context:\n",
      "\n",
      "textFile(name, minPartitions=None, use_unicode=True) method of pyspark.context.SparkContext instance\n",
      "    Read a text file from HDFS, a local file system (available on all\n",
      "    nodes), or any Hadoop-supported file system URI, and return it as an\n",
      "    RDD of Strings.\n",
      "    The text files must be encoded as UTF-8.\n",
      "    \n",
      "    If use_unicode is False, the strings will be kept as `str` (encoding\n",
      "    as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      "    Spark 1.2)\n",
      "    \n",
      "    >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      "    >>> with open(path, \"w\") as testFile:\n",
      "    ...    _ = testFile.write(\"Hello world!\")\n",
      "    >>> textFile = sc.textFile(path)\n",
      "    >>> textFile.collect()\n",
      "    ['Hello world!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pyspark/github_repo/Pyspark_Complete_Practice\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the 100th Etext file presented by Project Gutenberg, and',\n",
       " 'is presented in cooperation with World Library, Inc., from their',\n",
       " 'Library of the Future and Shakespeare CDROMS.  Project Gutenberg',\n",
       " 'often releases Etexts that are NOT placed in the Public Domain!!',\n",
       " '',\n",
       " 'Shakespeare',\n",
       " '',\n",
       " '*This Etext has certain copyright implications you should read!*',\n",
       " '',\n",
       " '<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM',\n",
       " 'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS',\n",
       " 'PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE',\n",
       " 'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE',\n",
       " 'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS',\n",
       " 'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED',\n",
       " 'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY',\n",
       " 'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>',\n",
       " '',\n",
       " '*Project Gutenberg is proud to cooperate with The World Library*',\n",
       " 'in the presentation of The Complete Works of William Shakespeare',\n",
       " 'for your reading for education and entertainment.  HOWEVER, THIS',\n",
       " 'IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY',\n",
       " 'OF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY',\n",
       " 'BE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!',\n",
       " 'TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!',\n",
       " '',\n",
       " '',\n",
       " '**Welcome To The World of Free Plain Vanilla Electronic Texts**',\n",
       " '',\n",
       " '**Etexts Readable By Both Humans and By Computers, Since 1971**',\n",
       " '',\n",
       " '*These Etexts Prepared By Hundreds of Volunteers and Donations*',\n",
       " '',\n",
       " 'Information on contacting Project Gutenberg to get Etexts, and',\n",
       " 'further information is included below.  We need your donations.',\n",
       " '',\n",
       " '',\n",
       " 'The Complete Works of William Shakespeare ',\n",
       " '',\n",
       " 'January, 1994  [Etext #100]',\n",
       " '',\n",
       " '',\n",
       " 'The Library of the Future Complete Works of William Shakespeare ',\n",
       " 'Library of the Future is a TradeMark (TM) of World Library Inc.',\n",
       " '******This file should be named shaks12.txt or shaks12.zip*****',\n",
       " '',\n",
       " 'Corrected EDITIONS of our etexts get a new NUMBER, shaks13.txt',\n",
       " 'VERSIONS based on separate sources get new LETTER, shaks10a.txt',\n",
       " '',\n",
       " 'If you would like further information about World Library, Inc.',\n",
       " 'Please call them at 1-800-443-0238 or email julianc@netcom.com',\n",
       " 'Please give them our thanks for their Shakespeare cooperation!',\n",
       " '',\n",
       " '',\n",
       " 'The official release date of all Project Gutenberg Etexts is at',\n",
       " 'Midnight, Central Time, of the last day of the stated month.  A',\n",
       " 'preliminary version may often be posted for suggestion, comment',\n",
       " 'and editing by those who wish to do so.  To be sure you have an',\n",
       " 'up to date first edition [xxxxx10x.xxx] please check file sizes',\n",
       " 'in the first week of the next month.  Since our ftp program has',\n",
       " 'a bug in it that scrambles the date [tried to fix and failed] a',\n",
       " 'look at the file size will have to do, but we will try to see a',\n",
       " 'new copy has at least one byte more or less.',\n",
       " '',\n",
       " '',\n",
       " 'Information about Project Gutenberg (one page)',\n",
       " '',\n",
       " 'We produce about two million dollars for each hour we work.  The',\n",
       " 'fifty hours is one conservative estimate for how long it we take',\n",
       " 'to get any etext selected, entered, proofread, edited, copyright',\n",
       " 'searched and analyzed, the copyright letters written, etc.  This',\n",
       " 'projected audience is one hundred million readers.  If our value',\n",
       " 'per text is nominally estimated at one dollar, then we produce 2',\n",
       " 'million dollars per hour this year we, will have to do four text',\n",
       " 'files per month:  thus upping our productivity from one million.',\n",
       " 'The Goal of Project Gutenberg is to Give Away One Trillion Etext',\n",
       " 'Files by the December 31, 2001.  [10,000 x 100,000,000=Trillion]',\n",
       " 'This is ten thousand titles each to one hundred million readers,',\n",
       " 'which is 10% of the expected number of computer users by the end',\n",
       " 'of the year 2001.',\n",
       " '',\n",
       " 'We need your donations more than ever!',\n",
       " '',\n",
       " 'All donations should be made to \"Project Gutenberg/IBC\", and are',\n",
       " 'tax deductible to the extent allowable by law (\"IBC\" is Illinois',\n",
       " 'Benedictine College).  (Subscriptions to our paper newsletter go',\n",
       " 'to IBC, too)',\n",
       " '',\n",
       " 'For these and other matters, please mail to:',\n",
       " '',\n",
       " 'Project Gutenberg',\n",
       " 'P. O. Box  2782',\n",
       " 'Champaign, IL 61825',\n",
       " '',\n",
       " 'When all other email fails try our Michael S. Hart, Executive Director:',\n",
       " 'hart@vmd.cso.uiuc.edu (internet)   hart@uiucvmd   (bitnet)',\n",
       " '',\n",
       " 'We would prefer to send you this information by email',\n",
       " '(Internet, Bitnet, Compuserve, ATTMAIL or MCImail).',\n",
       " '',\n",
       " '******',\n",
       " 'If you have an FTP program (or emulator), please',\n",
       " 'FTP directly to the Project Gutenberg archives:',\n",
       " '[Mac users, do NOT point and click. . .type]',\n",
       " '',\n",
       " 'ftp mrcnext.cso.uiuc.edu',\n",
       " 'login:  anonymous',\n",
       " 'password:  your@login',\n",
       " 'cd etext/etext91',\n",
       " 'or cd etext92',\n",
       " 'or cd etext93 [for new books]  [now also in cd etext/etext93]',\n",
       " 'or cd etext/articles [get suggest gut for more information]',\n",
       " 'dir [to see files]',\n",
       " 'get or mget [to get files. . .set bin for zip files]',\n",
       " 'GET 0INDEX.GUT',\n",
       " 'for a list of books',\n",
       " 'and',\n",
       " 'GET NEW GUT for general information',\n",
       " 'and',\n",
       " 'MGET GUT* for newsletters.',\n",
       " '',\n",
       " '**Information prepared by the Project Gutenberg legal advisor**',\n",
       " '',\n",
       " '',\n",
       " '***** SMALL PRINT! for COMPLETE SHAKESPEARE *****',\n",
       " '',\n",
       " 'THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM',\n",
       " 'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC.,',\n",
       " 'AND IS PROVIDED BY PROJECT GUTENBERG ETEXT OF',\n",
       " 'ILLINOIS BENEDICTINE COLLEGE WITH PERMISSION.',\n",
       " '',\n",
       " 'Since unlike many other Project Gutenberg-tm etexts, this etext',\n",
       " 'is copyright protected, and since the materials and methods you',\n",
       " \"use will effect the Project's reputation, your right to copy and\",\n",
       " 'distribute it is limited by the copyright and other laws, and by',\n",
       " 'the conditions of this \"Small Print!\" statement.',\n",
       " '',\n",
       " '1.  LICENSE',\n",
       " '',\n",
       " '  A) YOU MAY (AND ARE ENCOURAGED) TO DISTRIBUTE ELECTRONIC AND',\n",
       " 'MACHINE READABLE COPIES OF THIS ETEXT, SO LONG AS SUCH COPIES',\n",
       " '(1) ARE FOR YOUR OR OTHERS PERSONAL USE ONLY, AND (2) ARE NOT',\n",
       " 'DISTRIBUTED OR USED COMMERCIALLY.  PROHIBITED COMMERCIAL',\n",
       " 'DISTRIBUTION INCLUDES BY ANY SERVICE THAT CHARGES FOR DOWNLOAD',\n",
       " 'TIME OR FOR MEMBERSHIP.',\n",
       " '',\n",
       " '  B) This license is subject to the conditions that you honor',\n",
       " 'the refund and replacement provisions of this \"small print!\"',\n",
       " 'statement; and that you distribute exact copies of this etext,',\n",
       " 'including this Small Print statement.  Such copies can be',\n",
       " 'compressed or any proprietary form (including any form resulting',\n",
       " 'from word processing or hypertext software), so long as',\n",
       " '*EITHER*:',\n",
       " '',\n",
       " '    (1) The etext, when displayed, is clearly readable, and does',\n",
       " '  *not* contain characters other than those intended by the',\n",
       " '  author of the work, although tilde (~), asterisk (*) and',\n",
       " '  underline (_) characters may be used to convey punctuation',\n",
       " '  intended by the author, and additional characters may be used',\n",
       " '  to indicate hypertext links; OR',\n",
       " '',\n",
       " '    (2) The etext is readily convertible by the reader at no',\n",
       " '  expense into plain ASCII, EBCDIC or equivalent form by the',\n",
       " '  program that displays the etext (as is the case, for instance,',\n",
       " '  with most word processors); OR',\n",
       " '',\n",
       " '    (3) You provide or agree to provide on request at no',\n",
       " '  additional cost, fee or expense, a copy of the etext in plain',\n",
       " '  ASCII.',\n",
       " '',\n",
       " '2.  LIMITED WARRANTY; DISCLAIMER OF DAMAGES',\n",
       " '',\n",
       " 'This etext may contain a \"Defect\" in the form of incomplete,',\n",
       " 'inaccurate or corrupt data, transcription errors, a copyright or',\n",
       " 'other infringement, a defective or damaged disk, computer virus,',\n",
       " 'or codes that damage or cannot be read by your equipment.  But',\n",
       " 'for the \"Right of Replacement or Refund\" described below, the',\n",
       " 'Project (and any other party you may receive this etext from as',\n",
       " 'a PROJECT GUTENBERG-tm etext) disclaims all liability to you for',\n",
       " 'damages, costs and expenses, including legal fees, and YOU HAVE',\n",
       " 'NO REMEDIES FOR NEGLIGENCE OR UNDER STRICT LIABILITY, OR FOR',\n",
       " 'BREACH OF WARRANTY OR CONTRACT, INCLUDING BUT NOT LIMITED TO',\n",
       " 'INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES, EVEN IF',\n",
       " 'YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES.',\n",
       " '',\n",
       " 'If you discover a Defect in this etext within 90 days of receiv-',\n",
       " 'ing it, you can receive a refund of the money (if any) you paid',\n",
       " 'for it by sending an explanatory note within that time to the',\n",
       " 'person you received it from.  If you received it on a physical',\n",
       " 'medium, you must return it with your note, and such person may',\n",
       " 'choose to alternatively give you a replacement copy.  If you',\n",
       " 'received it electronically, such person may choose to',\n",
       " 'alternatively give you a second opportunity to receive it',\n",
       " 'electronically.',\n",
       " '',\n",
       " 'THIS ETEXT IS OTHERWISE PROVIDED TO YOU \"AS-IS\".  NO OTHER',\n",
       " 'WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, ARE MADE TO YOU AS',\n",
       " 'TO THE ETEXT OR ANY MEDIUM IT MAY BE ON, INCLUDING BUT NOT',\n",
       " 'LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A',\n",
       " 'PARTICULAR PURPOSE.  Some states do not allow disclaimers of',\n",
       " 'implied warranties or the exclusion or limitation of consequen-',\n",
       " 'tial damages, so the above disclaimers and exclusions may not',\n",
       " 'apply to you, and you may have other legal rights.',\n",
       " '',\n",
       " '3.  INDEMNITY: You will indemnify and hold the Project, its',\n",
       " 'directors, officers, members and agents harmless from all lia-',\n",
       " 'bility, cost and expense, including legal fees, that arise',\n",
       " 'directly or indirectly from any of the following that you do or',\n",
       " 'cause: [A] distribution of this etext, [B] alteration,',\n",
       " 'modification, or addition to the etext, or [C] any Defect.',\n",
       " '',\n",
       " \"4.  WHAT IF YOU *WANT* TO SEND MONEY EVEN IF YOU DON'T HAVE TO?\",\n",
       " 'Project Gutenberg is dedicated to increasing the number of',\n",
       " 'public domain and licensed works that can be freely distributed',\n",
       " 'in machine readable form.  The Project gratefully accepts',\n",
       " 'contributions in money, time, scanning machines, OCR software,',\n",
       " 'public domain etexts, royalty free copyright licenses, and',\n",
       " 'whatever else you can think of.  Money should be paid to \"Pro-',\n",
       " 'ject Gutenberg Association / Illinois Benedictine College\".',\n",
       " '',\n",
       " 'WRITE TO US! We can be reached at:',\n",
       " '     Internet: hart@vmd.cso.uiuc.edu',\n",
       " '       Bitnet: hart@uiucvmd',\n",
       " '   CompuServe: >internet:hart@.vmd.cso.uiuc.edu',\n",
       " '      Attmail: internet!vmd.cso.uiuc.edu!Hart',\n",
       " '        Mail:  Prof. Michael Hart',\n",
       " '               P.O. Box 2782',\n",
       " '               Champaign, IL 61825',\n",
       " '',\n",
       " 'This \"Small Print!\" by Charles B. Kramer, Attorney',\n",
       " 'Internet (72600.2026@compuserve.com); TEL: (212-254-5093)',\n",
       " '****   SMALL PRINT! FOR __ COMPLETE SHAKESPEARE ****',\n",
       " '[\"Small Print\" V.12.08.93]',\n",
       " '',\n",
       " '<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM         ',\n",
       " 'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS  ',\n",
       " 'PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE',\n",
       " 'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE    ',\n",
       " 'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS      ',\n",
       " 'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED             ',\n",
       " 'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY  ',\n",
       " 'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>> ',\n",
       " '',\n",
       " '',\n",
       " '1609',\n",
       " '',\n",
       " 'THE SONNETS',\n",
       " '',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '                     1',\n",
       " '  From fairest creatures we desire increase,',\n",
       " \"  That thereby beauty's rose might never die,\",\n",
       " '  But as the riper should by time decease,',\n",
       " '  His tender heir might bear his memory:',\n",
       " '  But thou contracted to thine own bright eyes,',\n",
       " \"  Feed'st thy light's flame with self-substantial fuel,\",\n",
       " '  Making a famine where abundance lies,',\n",
       " '  Thy self thy foe, to thy sweet self too cruel:',\n",
       " \"  Thou that art now the world's fresh ornament,\",\n",
       " '  And only herald to the gaudy spring,',\n",
       " '  Within thine own bud buriest thy content,',\n",
       " \"  And tender churl mak'st waste in niggarding:\",\n",
       " '    Pity the world, or else this glutton be,',\n",
       " \"    To eat the world's due, by the grave and thee.\",\n",
       " '',\n",
       " '',\n",
       " '                     2',\n",
       " '  When forty winters shall besiege thy brow,',\n",
       " \"  And dig deep trenches in thy beauty's field,\",\n",
       " \"  Thy youth's proud livery so gazed on now,\",\n",
       " '  Will be a tattered weed of small worth held:  ',\n",
       " '  Then being asked, where all thy beauty lies,',\n",
       " '  Where all the treasure of thy lusty days;',\n",
       " '  To say within thine own deep sunken eyes,',\n",
       " '  Were an all-eating shame, and thriftless praise.',\n",
       " \"  How much more praise deserved thy beauty's use,\",\n",
       " \"  If thou couldst answer 'This fair child of mine\",\n",
       " \"  Shall sum my count, and make my old excuse'\",\n",
       " '  Proving his beauty by succession thine.',\n",
       " '    This were to be new made when thou art old,',\n",
       " \"    And see thy blood warm when thou feel'st it cold.\",\n",
       " '',\n",
       " '',\n",
       " '                     3',\n",
       " '  Look in thy glass and tell the face thou viewest,',\n",
       " '  Now is the time that face should form another,',\n",
       " '  Whose fresh repair if now thou not renewest,',\n",
       " '  Thou dost beguile the world, unbless some mother.',\n",
       " '  For where is she so fair whose uneared womb',\n",
       " '  Disdains the tillage of thy husbandry?',\n",
       " '  Or who is he so fond will be the tomb,',\n",
       " '  Of his self-love to stop posterity?  ',\n",
       " \"  Thou art thy mother's glass and she in thee\",\n",
       " '  Calls back the lovely April of her prime,',\n",
       " '  So thou through windows of thine age shalt see,',\n",
       " '  Despite of wrinkles this thy golden time.',\n",
       " '    But if thou live remembered not to be,',\n",
       " '    Die single and thine image dies with thee.',\n",
       " '',\n",
       " '',\n",
       " '                     4',\n",
       " '  Unthrifty loveliness why dost thou spend,',\n",
       " \"  Upon thy self thy beauty's legacy?\",\n",
       " \"  Nature's bequest gives nothing but doth lend,\",\n",
       " '  And being frank she lends to those are free:',\n",
       " '  Then beauteous niggard why dost thou abuse,',\n",
       " '  The bounteous largess given thee to give?',\n",
       " '  Profitless usurer why dost thou use',\n",
       " '  So great a sum of sums yet canst not live?',\n",
       " '  For having traffic with thy self alone,',\n",
       " '  Thou of thy self thy sweet self dost deceive,',\n",
       " '  Then how when nature calls thee to be gone,',\n",
       " '  What acceptable audit canst thou leave?  ',\n",
       " '    Thy unused beauty must be tombed with thee,',\n",
       " \"    Which used lives th' executor to be.\",\n",
       " '',\n",
       " '',\n",
       " '                     5',\n",
       " '  Those hours that with gentle work did frame',\n",
       " '  The lovely gaze where every eye doth dwell',\n",
       " '  Will play the tyrants to the very same,',\n",
       " '  And that unfair which fairly doth excel:',\n",
       " '  For never-resting time leads summer on',\n",
       " '  To hideous winter and confounds him there,',\n",
       " '  Sap checked with frost and lusty leaves quite gone,',\n",
       " \"  Beauty o'er-snowed and bareness every where:\",\n",
       " \"  Then were not summer's distillation left\",\n",
       " '  A liquid prisoner pent in walls of glass,',\n",
       " \"  Beauty's effect with beauty were bereft,\",\n",
       " '  Nor it nor no remembrance what it was.',\n",
       " '    But flowers distilled though they with winter meet,',\n",
       " '    Leese but their show, their substance still lives sweet.',\n",
       " '',\n",
       " '',\n",
       " '                     6  ',\n",
       " \"  Then let not winter's ragged hand deface,\",\n",
       " '  In thee thy summer ere thou be distilled:',\n",
       " '  Make sweet some vial; treasure thou some place,',\n",
       " \"  With beauty's treasure ere it be self-killed:\",\n",
       " '  That use is not forbidden usury,',\n",
       " '  Which happies those that pay the willing loan;',\n",
       " \"  That's for thy self to breed another thee,\",\n",
       " '  Or ten times happier be it ten for one,',\n",
       " '  Ten times thy self were happier than thou art,',\n",
       " '  If ten of thine ten times refigured thee:',\n",
       " '  Then what could death do if thou shouldst depart,',\n",
       " '  Leaving thee living in posterity?',\n",
       " '    Be not self-willed for thou art much too fair,',\n",
       " \"    To be death's conquest and make worms thine heir.\",\n",
       " '',\n",
       " '',\n",
       " '                     7',\n",
       " '  Lo in the orient when the gracious light',\n",
       " '  Lifts up his burning head, each under eye',\n",
       " '  Doth homage to his new-appearing sight,',\n",
       " '  Serving with looks his sacred majesty,  ',\n",
       " '  And having climbed the steep-up heavenly hill,',\n",
       " '  Resembling strong youth in his middle age,',\n",
       " '  Yet mortal looks adore his beauty still,',\n",
       " '  Attending on his golden pilgrimage:',\n",
       " '  But when from highmost pitch with weary car,',\n",
       " '  Like feeble age he reeleth from the day,',\n",
       " '  The eyes (fore duteous) now converted are',\n",
       " '  From his low tract and look another way:',\n",
       " '    So thou, thy self out-going in thy noon:',\n",
       " '    Unlooked on diest unless thou get a son.',\n",
       " '',\n",
       " '',\n",
       " '                     8',\n",
       " \"  Music to hear, why hear'st thou music sadly?\",\n",
       " '  Sweets with sweets war not, joy delights in joy:',\n",
       " \"  Why lov'st thou that which thou receiv'st not gladly,\",\n",
       " \"  Or else receiv'st with pleasure thine annoy?\",\n",
       " '  If the true concord of well-tuned sounds,',\n",
       " '  By unions married do offend thine ear,',\n",
       " '  They do but sweetly chide thee, who confounds',\n",
       " '  In singleness the parts that thou shouldst bear:  ',\n",
       " '  Mark how one string sweet husband to another,',\n",
       " '  Strikes each in each by mutual ordering;',\n",
       " '  Resembling sire, and child, and happy mother,',\n",
       " '  Who all in one, one pleasing note do sing:',\n",
       " '    Whose speechless song being many, seeming one,',\n",
       " \"    Sings this to thee, 'Thou single wilt prove none'.\",\n",
       " '',\n",
       " '',\n",
       " '                     9',\n",
       " \"  Is it for fear to wet a widow's eye,\",\n",
       " \"  That thou consum'st thy self in single life?\",\n",
       " '  Ah, if thou issueless shalt hap to die,',\n",
       " '  The world will wail thee like a makeless wife,',\n",
       " '  The world will be thy widow and still weep,',\n",
       " '  That thou no form of thee hast left behind,',\n",
       " '  When every private widow well may keep,',\n",
       " \"  By children's eyes, her husband's shape in mind:\",\n",
       " '  Look what an unthrift in the world doth spend',\n",
       " '  Shifts but his place, for still the world enjoys it;',\n",
       " \"  But beauty's waste hath in the world an end,\",\n",
       " '  And kept unused the user so destroys it:  ',\n",
       " '    No love toward others in that bosom sits',\n",
       " \"    That on himself such murd'rous shame commits.\",\n",
       " '',\n",
       " '',\n",
       " '                     10',\n",
       " \"  For shame deny that thou bear'st love to any\",\n",
       " '  Who for thy self art so unprovident.',\n",
       " '  Grant if thou wilt, thou art beloved of many,',\n",
       " \"  But that thou none lov'st is most evident:\",\n",
       " \"  For thou art so possessed with murd'rous hate,\",\n",
       " \"  That 'gainst thy self thou stick'st not to conspire,\",\n",
       " '  Seeking that beauteous roof to ruinate',\n",
       " '  Which to repair should be thy chief desire:',\n",
       " '  O change thy thought, that I may change my mind,',\n",
       " '  Shall hate be fairer lodged than gentle love?',\n",
       " '  Be as thy presence is gracious and kind,',\n",
       " '  Or to thy self at least kind-hearted prove,',\n",
       " '    Make thee another self for love of me,',\n",
       " '    That beauty still may live in thine or thee.',\n",
       " '',\n",
       " '',\n",
       " '                     11  ',\n",
       " \"  As fast as thou shalt wane so fast thou grow'st,\",\n",
       " '  In one of thine, from that which thou departest,',\n",
       " \"  And that fresh blood which youngly thou bestow'st,\",\n",
       " '  Thou mayst call thine, when thou from youth convertest,',\n",
       " '  Herein lives wisdom, beauty, and increase,',\n",
       " '  Without this folly, age, and cold decay,',\n",
       " '  If all were minded so, the times should cease,',\n",
       " '  And threescore year would make the world away:',\n",
       " '  Let those whom nature hath not made for store,',\n",
       " '  Harsh, featureless, and rude, barrenly perish:',\n",
       " '  Look whom she best endowed, she gave thee more;',\n",
       " '  Which bounteous gift thou shouldst in bounty cherish:',\n",
       " '    She carved thee for her seal, and meant thereby,',\n",
       " '    Thou shouldst print more, not let that copy die.',\n",
       " '',\n",
       " '',\n",
       " '                     12',\n",
       " '  When I do count the clock that tells the time,',\n",
       " '  And see the brave day sunk in hideous night,',\n",
       " '  When I behold the violet past prime,',\n",
       " \"  And sable curls all silvered o'er with white:  \",\n",
       " '  When lofty trees I see barren of leaves,',\n",
       " '  Which erst from heat did canopy the herd',\n",
       " \"  And summer's green all girded up in sheaves\",\n",
       " '  Borne on the bier with white and bristly beard:',\n",
       " '  Then of thy beauty do I question make',\n",
       " '  That thou among the wastes of time must go,',\n",
       " '  Since sweets and beauties do themselves forsake,',\n",
       " '  And die as fast as they see others grow,',\n",
       " \"    And nothing 'gainst Time's scythe can make defence\",\n",
       " '    Save breed to brave him, when he takes thee hence.',\n",
       " '',\n",
       " '',\n",
       " '                     13',\n",
       " '  O that you were your self, but love you are',\n",
       " '  No longer yours, than you your self here live,',\n",
       " '  Against this coming end you should prepare,',\n",
       " '  And your sweet semblance to some other give.',\n",
       " '  So should that beauty which you hold in lease',\n",
       " '  Find no determination, then you were',\n",
       " \"  Your self again after your self's decease,\",\n",
       " '  When your sweet issue your sweet form should bear.  ',\n",
       " '  Who lets so fair a house fall to decay,',\n",
       " '  Which husbandry in honour might uphold,',\n",
       " \"  Against the stormy gusts of winter's day\",\n",
       " \"  And barren rage of death's eternal cold?\",\n",
       " '    O none but unthrifts, dear my love you know,',\n",
       " '    You had a father, let your son say so.',\n",
       " '',\n",
       " '',\n",
       " '                     14',\n",
       " '  Not from the stars do I my judgement pluck,',\n",
       " '  And yet methinks I have astronomy,',\n",
       " '  But not to tell of good, or evil luck,',\n",
       " \"  Of plagues, of dearths, or seasons' quality,\",\n",
       " '  Nor can I fortune to brief minutes tell;',\n",
       " '  Pointing to each his thunder, rain and wind,',\n",
       " '  Or say with princes if it shall go well',\n",
       " '  By oft predict that I in heaven find.',\n",
       " '  But from thine eyes my knowledge I derive,',\n",
       " '  And constant stars in them I read such art',\n",
       " '  As truth and beauty shall together thrive',\n",
       " '  If from thy self, to store thou wouldst convert:  ',\n",
       " '    Or else of thee this I prognosticate,',\n",
       " \"    Thy end is truth's and beauty's doom and date.\",\n",
       " '',\n",
       " '',\n",
       " '                     15',\n",
       " '  When I consider every thing that grows',\n",
       " '  Holds in perfection but a little moment.',\n",
       " '  That this huge stage presenteth nought but shows',\n",
       " '  Whereon the stars in secret influence comment.',\n",
       " '  When I perceive that men as plants increase,',\n",
       " '  Cheered and checked even by the self-same sky:',\n",
       " '  Vaunt in their youthful sap, at height decrease,',\n",
       " '  And wear their brave state out of memory.',\n",
       " '  Then the conceit of this inconstant stay,',\n",
       " '  Sets you most rich in youth before my sight,',\n",
       " '  Where wasteful time debateth with decay',\n",
       " '  To change your day of youth to sullied night,',\n",
       " '    And all in war with Time for love of you,',\n",
       " '    As he takes from you, I engraft you new.',\n",
       " '',\n",
       " '',\n",
       " '                     16  ',\n",
       " '  But wherefore do not you a mightier way',\n",
       " '  Make war upon this bloody tyrant Time?',\n",
       " '  And fortify your self in your decay',\n",
       " '  With means more blessed than my barren rhyme?',\n",
       " '  Now stand you on the top of happy hours,',\n",
       " '  And many maiden gardens yet unset,',\n",
       " '  With virtuous wish would bear you living flowers,',\n",
       " '  Much liker than your painted counterfeit:',\n",
       " '  So should the lines of life that life repair',\n",
       " \"  Which this (Time's pencil) or my pupil pen\",\n",
       " '  Neither in inward worth nor outward fair',\n",
       " '  Can make you live your self in eyes of men.',\n",
       " '    To give away your self, keeps your self still,',\n",
       " '    And you must live drawn by your own sweet skill.',\n",
       " '',\n",
       " '',\n",
       " '                     17',\n",
       " '  Who will believe my verse in time to come',\n",
       " '  If it were filled with your most high deserts?',\n",
       " '  Though yet heaven knows it is but as a tomb',\n",
       " '  Which hides your life, and shows not half your parts:  ',\n",
       " '  If I could write the beauty of your eyes,',\n",
       " '  And in fresh numbers number all your graces,',\n",
       " '  The age to come would say this poet lies,',\n",
       " \"  Such heavenly touches ne'er touched earthly faces.\",\n",
       " '  So should my papers (yellowed with their age)',\n",
       " '  Be scorned, like old men of less truth than tongue,',\n",
       " \"  And your true rights be termed a poet's rage,\",\n",
       " '  And stretched metre of an antique song.',\n",
       " '    But were some child of yours alive that time,',\n",
       " '    You should live twice in it, and in my rhyme.',\n",
       " '',\n",
       " '',\n",
       " '                     18',\n",
       " \"  Shall I compare thee to a summer's day?\",\n",
       " '  Thou art more lovely and more temperate:',\n",
       " '  Rough winds do shake the darling buds of May,',\n",
       " \"  And summer's lease hath all too short a date:\",\n",
       " '  Sometime too hot the eye of heaven shines,',\n",
       " '  And often is his gold complexion dimmed,',\n",
       " '  And every fair from fair sometime declines,',\n",
       " \"  By chance, or nature's changing course untrimmed:  \",\n",
       " '  But thy eternal summer shall not fade,',\n",
       " \"  Nor lose possession of that fair thou ow'st,\",\n",
       " \"  Nor shall death brag thou wand'rest in his shade,\",\n",
       " \"  When in eternal lines to time thou grow'st,\",\n",
       " '    So long as men can breathe or eyes can see,',\n",
       " '    So long lives this, and this gives life to thee.',\n",
       " '',\n",
       " '',\n",
       " '                     19',\n",
       " \"  Devouring Time blunt thou the lion's paws,\",\n",
       " '  And make the earth devour her own sweet brood,',\n",
       " \"  Pluck the keen teeth from the fierce tiger's jaws,\",\n",
       " '  And burn the long-lived phoenix, in her blood,',\n",
       " \"  Make glad and sorry seasons as thou fleet'st,\",\n",
       " \"  And do whate'er thou wilt swift-footed Time\",\n",
       " '  To the wide world and all her fading sweets:',\n",
       " '  But I forbid thee one most heinous crime,',\n",
       " \"  O carve not with thy hours my love's fair brow,\",\n",
       " '  Nor draw no lines there with thine antique pen,',\n",
       " '  Him in thy course untainted do allow,',\n",
       " \"  For beauty's pattern to succeeding men.  \",\n",
       " '    Yet do thy worst old Time: despite thy wrong,',\n",
       " '    My love shall in my verse ever live young.',\n",
       " '',\n",
       " '',\n",
       " '                     20',\n",
       " \"  A woman's face with nature's own hand painted,\",\n",
       " '  Hast thou the master mistress of my passion,',\n",
       " \"  A woman's gentle heart but not acquainted\",\n",
       " \"  With shifting change as is false women's fashion,\",\n",
       " '  An eye more bright than theirs, less false in rolling:',\n",
       " '  Gilding the object whereupon it gazeth,',\n",
       " '  A man in hue all hues in his controlling,',\n",
       " \"  Which steals men's eyes and women's souls amazeth.\",\n",
       " '  And for a woman wert thou first created,',\n",
       " '  Till nature as she wrought thee fell a-doting,',\n",
       " '  And by addition me of thee defeated,',\n",
       " '  By adding one thing to my purpose nothing.',\n",
       " \"    But since she pricked thee out for women's pleasure,\",\n",
       " \"    Mine be thy love and thy love's use their treasure.\",\n",
       " '',\n",
       " '',\n",
       " '                     21  ',\n",
       " '  So is it not with me as with that muse,',\n",
       " '  Stirred by a painted beauty to his verse,',\n",
       " '  Who heaven it self for ornament doth use,',\n",
       " '  And every fair with his fair doth rehearse,',\n",
       " '  Making a couplement of proud compare',\n",
       " \"  With sun and moon, with earth and sea's rich gems:\",\n",
       " \"  With April's first-born flowers and all things rare,\",\n",
       " \"  That heaven's air in this huge rondure hems.\",\n",
       " '  O let me true in love but truly write,',\n",
       " '  And then believe me, my love is as fair,',\n",
       " \"  As any mother's child, though not so bright\",\n",
       " \"  As those gold candles fixed in heaven's air:\",\n",
       " '    Let them say more that like of hearsay well,',\n",
       " '    I will not praise that purpose not to sell.',\n",
       " '',\n",
       " '',\n",
       " '                     22',\n",
       " '  My glass shall not persuade me I am old,',\n",
       " '  So long as youth and thou are of one date,',\n",
       " \"  But when in thee time's furrows I behold,\",\n",
       " '  Then look I death my days should expiate.  ',\n",
       " '  For all that beauty that doth cover thee,',\n",
       " '  Is but the seemly raiment of my heart,',\n",
       " '  Which in thy breast doth live, as thine in me,',\n",
       " '  How can I then be elder than thou art?',\n",
       " '  O therefore love be of thyself so wary,',\n",
       " '  As I not for my self, but for thee will,',\n",
       " '  Bearing thy heart which I will keep so chary',\n",
       " '  As tender nurse her babe from faring ill.',\n",
       " '    Presume not on thy heart when mine is slain,',\n",
       " \"    Thou gav'st me thine not to give back again.\",\n",
       " '',\n",
       " '',\n",
       " '                     23',\n",
       " '  As an unperfect actor on the stage,',\n",
       " '  Who with his fear is put beside his part,',\n",
       " '  Or some fierce thing replete with too much rage,',\n",
       " \"  Whose strength's abundance weakens his own heart;\",\n",
       " '  So I for fear of trust, forget to say,',\n",
       " \"  The perfect ceremony of love's rite,\",\n",
       " \"  And in mine own love's strength seem to decay,\",\n",
       " \"  O'ercharged with burthen of mine own love's might:  \",\n",
       " '  O let my looks be then the eloquence,',\n",
       " '  And dumb presagers of my speaking breast,',\n",
       " '  Who plead for love, and look for recompense,',\n",
       " '  More than that tongue that more hath more expressed.',\n",
       " '    O learn to read what silent love hath writ,',\n",
       " \"    To hear with eyes belongs to love's fine wit.\",\n",
       " '',\n",
       " '',\n",
       " '                     24',\n",
       " '  Mine eye hath played the painter and hath stelled,',\n",
       " \"  Thy beauty's form in table of my heart,\",\n",
       " \"  My body is the frame wherein 'tis held,\",\n",
       " \"  And perspective it is best painter's art.\",\n",
       " '  For through the painter must you see his skill,',\n",
       " '  To find where your true image pictured lies,',\n",
       " \"  Which in my bosom's shop is hanging still,\",\n",
       " '  That hath his windows glazed with thine eyes:',\n",
       " '  Now see what good turns eyes for eyes have done,',\n",
       " '  Mine eyes have drawn thy shape, and thine for me',\n",
       " '  Are windows to my breast, where-through the sun',\n",
       " '  Delights to peep, to gaze therein on thee;  ',\n",
       " '    Yet eyes this cunning want to grace their art,',\n",
       " '    They draw but what they see, know not the heart.',\n",
       " '',\n",
       " '',\n",
       " '                     25',\n",
       " '  Let those who are in favour with their stars,',\n",
       " '  Of public honour and proud titles boast,',\n",
       " '  Whilst I whom fortune of such triumph bars',\n",
       " '  Unlooked for joy in that I honour most;',\n",
       " \"  Great princes' favourites their fair leaves spread,\",\n",
       " \"  But as the marigold at the sun's eye,\",\n",
       " '  And in themselves their pride lies buried,',\n",
       " '  For at a frown they in their glory die.',\n",
       " '  The painful warrior famoused for fight,',\n",
       " '  After a thousand victories once foiled,',\n",
       " '  Is from the book of honour razed quite,',\n",
       " '  And all the rest forgot for which he toiled:',\n",
       " '    Then happy I that love and am beloved',\n",
       " '    Where I may not remove nor be removed.',\n",
       " '',\n",
       " '',\n",
       " '                     26  ',\n",
       " '  Lord of my love, to whom in vassalage',\n",
       " '  Thy merit hath my duty strongly knit;',\n",
       " '  To thee I send this written embassage',\n",
       " '  To witness duty, not to show my wit.',\n",
       " '  Duty so great, which wit so poor as mine',\n",
       " '  May make seem bare, in wanting words to show it;',\n",
       " '  But that I hope some good conceit of thine',\n",
       " \"  In thy soul's thought (all naked) will bestow it:\",\n",
       " '  Till whatsoever star that guides my moving,',\n",
       " '  Points on me graciously with fair aspect,',\n",
       " '  And puts apparel on my tattered loving,',\n",
       " '  To show me worthy of thy sweet respect,',\n",
       " '    Then may I dare to boast how I do love thee,',\n",
       " '    Till then, not show my head where thou mayst prove me.',\n",
       " '',\n",
       " '',\n",
       " '                     27',\n",
       " '  Weary with toil, I haste me to my bed,',\n",
       " '  The dear respose for limbs with travel tired,',\n",
       " '  But then begins a journey in my head',\n",
       " \"  To work my mind, when body's work's expired.  \",\n",
       " '  For then my thoughts (from far where I abide)',\n",
       " '  Intend a zealous pilgrimage to thee,',\n",
       " '  And keep my drooping eyelids open wide,',\n",
       " '  Looking on darkness which the blind do see.',\n",
       " \"  Save that my soul's imaginary sight\",\n",
       " '  Presents thy shadow to my sightless view,',\n",
       " '  Which like a jewel (hung in ghastly night)',\n",
       " '  Makes black night beauteous, and her old face new.',\n",
       " '    Lo thus by day my limbs, by night my mind,',\n",
       " '    For thee, and for my self, no quiet find.',\n",
       " '',\n",
       " '',\n",
       " '                     28',\n",
       " '  How can I then return in happy plight',\n",
       " '  That am debarred the benefit of rest?',\n",
       " \"  When day's oppression is not eased by night,\",\n",
       " '  But day by night and night by day oppressed.',\n",
       " \"  And each (though enemies to either's reign)\",\n",
       " '  Do in consent shake hands to torture me,',\n",
       " '  The one by toil, the other to complain',\n",
       " '  How far I toil, still farther off from thee.  ',\n",
       " '  I tell the day to please him thou art bright,',\n",
       " '  And dost him grace when clouds do blot the heaven:',\n",
       " '  So flatter I the swart-complexioned night,',\n",
       " \"  When sparkling stars twire not thou gild'st the even.\",\n",
       " '    But day doth daily draw my sorrows longer,',\n",
       " \"    And night doth nightly make grief's length seem stronger\",\n",
       " '',\n",
       " '',\n",
       " '                     29',\n",
       " \"  When in disgrace with Fortune and men's eyes,\",\n",
       " '  I all alone beweep my outcast state,',\n",
       " '  And trouble deaf heaven with my bootless cries,',\n",
       " '  And look upon my self and curse my fate,',\n",
       " '  Wishing me like to one more rich in hope,',\n",
       " '  Featured like him, like him with friends possessed,',\n",
       " \"  Desiring this man's art, and that man's scope,\",\n",
       " '  With what I most enjoy contented least,',\n",
       " '  Yet in these thoughts my self almost despising,',\n",
       " '  Haply I think on thee, and then my state,',\n",
       " '  (Like to the lark at break of day arising',\n",
       " \"  From sullen earth) sings hymns at heaven's gate,  \",\n",
       " '    For thy sweet love remembered such wealth brings,',\n",
       " '    That then I scorn to change my state with kings.',\n",
       " '',\n",
       " '',\n",
       " '                     30',\n",
       " '  When to the sessions of sweet silent thought,',\n",
       " '  I summon up remembrance of things past,',\n",
       " '  I sigh the lack of many a thing I sought,',\n",
       " \"  And with old woes new wail my dear time's waste:\",\n",
       " '  Then can I drown an eye (unused to flow)',\n",
       " \"  For precious friends hid in death's dateless night,\",\n",
       " \"  And weep afresh love's long since cancelled woe,\",\n",
       " \"  And moan th' expense of many a vanished sight.\",\n",
       " '  Then can I grieve at grievances foregone,',\n",
       " \"  And heavily from woe to woe tell o'er\",\n",
       " '  The sad account of fore-bemoaned moan,',\n",
       " '  Which I new pay as if not paid before.',\n",
       " '    But if the while I think on thee (dear friend)',\n",
       " '    All losses are restored, and sorrows end.',\n",
       " '',\n",
       " '',\n",
       " '                     31  ',\n",
       " '  Thy bosom is endeared with all hearts,',\n",
       " '  Which I by lacking have supposed dead,',\n",
       " \"  And there reigns love and all love's loving parts,\",\n",
       " '  And all those friends which I thought buried.',\n",
       " '  How many a holy and obsequious tear',\n",
       " \"  Hath dear religious love stol'n from mine eye,\",\n",
       " '  As interest of the dead, which now appear,',\n",
       " '  But things removed that hidden in thee lie.',\n",
       " '  Thou art the grave where buried love doth live,',\n",
       " '  Hung with the trophies of my lovers gone,',\n",
       " '  Who all their parts of me to thee did give,',\n",
       " '  That due of many, now is thine alone.',\n",
       " '    Their images I loved, I view in thee,',\n",
       " '    And thou (all they) hast all the all of me.',\n",
       " '',\n",
       " '',\n",
       " '                     32',\n",
       " '  If thou survive my well-contented day,',\n",
       " '  When that churl death my bones with dust shall cover',\n",
       " '  And shalt by fortune once more re-survey',\n",
       " '  These poor rude lines of thy deceased lover:  ',\n",
       " \"  Compare them with the bett'ring of the time,\",\n",
       " '  And though they be outstripped by every pen,',\n",
       " '  Reserve them for my love, not for their rhyme,',\n",
       " '  Exceeded by the height of happier men.',\n",
       " '  O then vouchsafe me but this loving thought,',\n",
       " \"  'Had my friend's Muse grown with this growing age,\",\n",
       " '  A dearer birth than this his love had brought',\n",
       " '  To march in ranks of better equipage:',\n",
       " '    But since he died and poets better prove,',\n",
       " \"    Theirs for their style I'll read, his for his love'.\",\n",
       " '',\n",
       " '',\n",
       " '                     33',\n",
       " '  Full many a glorious morning have I seen,',\n",
       " '  Flatter the mountain tops with sovereign eye,',\n",
       " '  Kissing with golden face the meadows green;',\n",
       " '  Gilding pale streams with heavenly alchemy:',\n",
       " '  Anon permit the basest clouds to ride,',\n",
       " '  With ugly rack on his celestial face,',\n",
       " '  And from the forlorn world his visage hide',\n",
       " '  Stealing unseen to west with this disgrace:  ',\n",
       " '  Even so my sun one early morn did shine,',\n",
       " '  With all triumphant splendour on my brow,',\n",
       " '  But out alack, he was but one hour mine,',\n",
       " '  The region cloud hath masked him from me now.',\n",
       " '    Yet him for this, my love no whit disdaineth,',\n",
       " \"    Suns of the world may stain, when heaven's sun staineth.\",\n",
       " '',\n",
       " '',\n",
       " '                     34',\n",
       " '  Why didst thou promise such a beauteous day,',\n",
       " '  And make me travel forth without my cloak,',\n",
       " \"  To let base clouds o'ertake me in my way,\",\n",
       " \"  Hiding thy brav'ry in their rotten smoke?\",\n",
       " \"  'Tis not enough that through the cloud thou break,\",\n",
       " '  To dry the rain on my storm-beaten face,',\n",
       " '  For no man well of such a salve can speak,',\n",
       " '  That heals the wound, and cures not the disgrace:',\n",
       " '  Nor can thy shame give physic to my grief,',\n",
       " '  Though thou repent, yet I have still the loss,',\n",
       " \"  Th' offender's sorrow lends but weak relief\",\n",
       " \"  To him that bears the strong offence's cross.  \",\n",
       " '    Ah but those tears are pearl which thy love sheds,',\n",
       " '    And they are rich, and ransom all ill deeds.',\n",
       " '',\n",
       " '',\n",
       " '                     35',\n",
       " '  No more be grieved at that which thou hast done,',\n",
       " '  Roses have thorns, and silver fountains mud,',\n",
       " '  Clouds and eclipses stain both moon and sun,',\n",
       " '  And loathsome canker lives in sweetest bud.',\n",
       " '  All men make faults, and even I in this,',\n",
       " '  Authorizing thy trespass with compare,',\n",
       " '  My self corrupting salving thy amiss,',\n",
       " '  Excusing thy sins more than thy sins are:',\n",
       " '  For to thy sensual fault I bring in sense,',\n",
       " '  Thy adverse party is thy advocate,',\n",
       " \"  And 'gainst my self a lawful plea commence:\",\n",
       " '  Such civil war is in my love and hate,',\n",
       " '    That I an accessary needs must be,',\n",
       " '    To that sweet thief which sourly robs from me.',\n",
       " '',\n",
       " '',\n",
       " '                     36  ',\n",
       " '  Let me confess that we two must be twain,',\n",
       " '  Although our undivided loves are one:',\n",
       " '  So shall those blots that do with me remain,',\n",
       " '  Without thy help, by me be borne alone.',\n",
       " '  In our two loves there is but one respect,',\n",
       " '  Though in our lives a separable spite,',\n",
       " \"  Which though it alter not love's sole effect,\",\n",
       " \"  Yet doth it steal sweet hours from love's delight.\",\n",
       " '  I may not evermore acknowledge thee,',\n",
       " '  Lest my bewailed guilt should do thee shame,',\n",
       " '  Nor thou with public kindness honour me,',\n",
       " '  Unless thou take that honour from thy name:',\n",
       " '    But do not so, I love thee in such sort,',\n",
       " '    As thou being mine, mine is thy good report.',\n",
       " '',\n",
       " '',\n",
       " '                     37',\n",
       " '  As a decrepit father takes delight,',\n",
       " '  To see his active child do deeds of youth,',\n",
       " \"  So I, made lame by Fortune's dearest spite\",\n",
       " '  Take all my comfort of thy worth and truth.  ',\n",
       " '  For whether beauty, birth, or wealth, or wit,',\n",
       " '  Or any of these all, or all, or more',\n",
       " '  Entitled in thy parts, do crowned sit,',\n",
       " '  I make my love engrafted to this store:',\n",
       " '  So then I am not lame, poor, nor despised,',\n",
       " '  Whilst that this shadow doth such substance give,',\n",
       " '  That I in thy abundance am sufficed,',\n",
       " '  And by a part of all thy glory live:',\n",
       " '    Look what is best, that best I wish in thee,',\n",
       " '    This wish I have, then ten times happy me.',\n",
       " '',\n",
       " '',\n",
       " '                     38',\n",
       " '  How can my muse want subject to invent',\n",
       " \"  While thou dost breathe that pour'st into my verse,\",\n",
       " '  Thine own sweet argument, too excellent,',\n",
       " '  For every vulgar paper to rehearse?',\n",
       " '  O give thy self the thanks if aught in me,',\n",
       " '  Worthy perusal stand against thy sight,',\n",
       " \"  For who's so dumb that cannot write to thee,\",\n",
       " '  When thou thy self dost give invention light?  ',\n",
       " '  Be thou the tenth Muse, ten times more in worth',\n",
       " '  Than those old nine which rhymers invocate,',\n",
       " '  And he that calls on thee, let him bring forth',\n",
       " '  Eternal numbers to outlive long date.',\n",
       " '    If my slight muse do please these curious days,',\n",
       " '    The pain be mine, but thine shall be the praise.',\n",
       " '',\n",
       " '',\n",
       " '                     39',\n",
       " '  O how thy worth with manners may I sing,',\n",
       " '  When thou art all the better part of me?',\n",
       " '  What can mine own praise to mine own self bring:',\n",
       " \"  And what is't but mine own when I praise thee?\",\n",
       " '  Even for this, let us divided live,',\n",
       " '  And our dear love lose name of single one,',\n",
       " '  That by this separation I may give:',\n",
       " \"  That due to thee which thou deserv'st alone:\",\n",
       " '  O absence what a torment wouldst thou prove,',\n",
       " '  Were it not thy sour leisure gave sweet leave,',\n",
       " '  To entertain the time with thoughts of love,',\n",
       " '  Which time and thoughts so sweetly doth deceive.  ',\n",
       " '    And that thou teachest how to make one twain,',\n",
       " '    By praising him here who doth hence remain.',\n",
       " '',\n",
       " '',\n",
       " '                     40',\n",
       " '  Take all my loves, my love, yea take them all,',\n",
       " '  What hast thou then more than thou hadst before?',\n",
       " '  No love, my love, that thou mayst true love call,',\n",
       " '  All mine was thine, before thou hadst this more:',\n",
       " '  Then if for my love, thou my love receivest,',\n",
       " '  I cannot blame thee, for my love thou usest,',\n",
       " '  But yet be blamed, if thou thy self deceivest',\n",
       " '  By wilful taste of what thy self refusest.',\n",
       " '  I do forgive thy robbery gentle thief',\n",
       " '  Although thou steal thee all my poverty:',\n",
       " '  And yet love knows it is a greater grief',\n",
       " \"  To bear greater wrong, than hate's known injury.\",\n",
       " '    Lascivious grace, in whom all ill well shows,',\n",
       " '    Kill me with spites yet we must not be foes.',\n",
       " '',\n",
       " '',\n",
       " '                     41  ',\n",
       " '  Those pretty wrongs that liberty commits,',\n",
       " '  When I am sometime absent from thy heart,',\n",
       " '  Thy beauty, and thy years full well befits,',\n",
       " '  For still temptation follows where thou art.',\n",
       " '  Gentle thou art, and therefore to be won,',\n",
       " '  Beauteous thou art, therefore to be assailed.',\n",
       " \"  And when a woman woos, what woman's son,\",\n",
       " '  Will sourly leave her till he have prevailed?',\n",
       " '  Ay me, but yet thou mightst my seat forbear,',\n",
       " '  And chide thy beauty, and thy straying youth,',\n",
       " '  Who lead thee in their riot even there',\n",
       " '  Where thou art forced to break a twofold truth:',\n",
       " '    Hers by thy beauty tempting her to thee,',\n",
       " '    Thine by thy beauty being false to me.',\n",
       " '',\n",
       " '',\n",
       " '                     42',\n",
       " '  That thou hast her it is not all my grief,',\n",
       " '  And yet it may be said I loved her dearly,',\n",
       " '  That she hath thee is of my wailing chief,',\n",
       " '  A loss in love that touches me more nearly.  ',\n",
       " '  Loving offenders thus I will excuse ye,',\n",
       " \"  Thou dost love her, because thou know'st I love her,\",\n",
       " '  And for my sake even so doth she abuse me,',\n",
       " \"  Suff'ring my friend for my sake to approve her.\",\n",
       " \"  If I lose thee, my loss is my love's gain,\",\n",
       " '  And losing her, my friend hath found that loss,',\n",
       " '  Both find each other, and I lose both twain,',\n",
       " '  And both for my sake lay on me this cross,',\n",
       " \"    But here's the joy, my friend and I are one,\",\n",
       " '    Sweet flattery, then she loves but me alone.',\n",
       " '',\n",
       " '',\n",
       " '                     43',\n",
       " '  When most I wink then do mine eyes best see,',\n",
       " '  For all the day they view things unrespected,',\n",
       " '  But when I sleep, in dreams they look on thee,',\n",
       " '  And darkly bright, are bright in dark directed.',\n",
       " '  Then thou whose shadow shadows doth make bright',\n",
       " \"  How would thy shadow's form, form happy show,\",\n",
       " '  To the clear day with thy much clearer light,',\n",
       " '  When to unseeing eyes thy shade shines so!  ',\n",
       " '  How would (I say) mine eyes be blessed made,',\n",
       " '  By looking on thee in the living day,',\n",
       " '  When in dead night thy fair imperfect shade,',\n",
       " '  Through heavy sleep on sightless eyes doth stay!',\n",
       " '    All days are nights to see till I see thee,',\n",
       " '    And nights bright days when dreams do show thee me.',\n",
       " '',\n",
       " '',\n",
       " '                     44',\n",
       " '  If the dull substance of my flesh were thought,',\n",
       " '  Injurious distance should not stop my way,',\n",
       " '  For then despite of space I would be brought,',\n",
       " '  From limits far remote, where thou dost stay,',\n",
       " '  No matter then although my foot did stand',\n",
       " '  Upon the farthest earth removed from thee,',\n",
       " '  For nimble thought can jump both sea and land,',\n",
       " '  As soon as think the place where he would be.',\n",
       " '  But ah, thought kills me that I am not thought',\n",
       " '  To leap large lengths of miles when thou art gone,',\n",
       " '  But that so much of earth and water wrought,',\n",
       " \"  I must attend, time's leisure with my moan.  \",\n",
       " '    Receiving nought by elements so slow,',\n",
       " \"    But heavy tears, badges of either's woe.\",\n",
       " '',\n",
       " '',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = sc.textFile(r'shakespeare.txt')\n",
    "text_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(builtins.object)\n",
      " |  RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |  \n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  barrier(self)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
      " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
      " |      entire stage and relaunch all tasks for this stage.\n",
      " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
      " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
      " |      \n",
      " |      :return: an :class:`RDDBarrier` instance that provides actions within a barrier stage.\n",
      " |      \n",
      " |      .. seealso:: :class:`BarrierTaskContext`\n",
      " |      .. seealso:: `SPIP: Barrier Execution Mode\n",
      " |          <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
      " |      .. seealso:: `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
      " |      ``b`` is in `other`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in `self` or `other`, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in `self` as\n",
      " |      well as `other`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting data is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - `createCombiner`, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n",
      " |            the lists)\n",
      " |      \n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      .. note:: V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      :param relativeSD: Relative accuracy. Smaller values create\n",
      " |                         counters that require more space.\n",
      " |                         It must be greater than 0.000017.\n",
      " |      \n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
      " |      (k, (None, w)) if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      .. note:: If you are grouping in order to perform an aggregation (such as a\n",
      " |          sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |          provide much better performance.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) inseration to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      The return value is a tuple of buckets and histogram.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      .. note:: This method performs a shuffle internally.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      .. note:: an RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self)\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      `self` and `other`.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying `f`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self)\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Deprecated: use mapPartitionsWithIndex instead.\n",
      " |      \n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |      \n",
      " |      :param checkCode: whether or not to check the return value of the shell command.\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      " |      :param seed: random seed\n",
      " |      :return: split RDDs in a list\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>)\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with `numPartitions` partitions, or\n",
      " |      the default parallelism level if `numPartitions` is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |      Can increase or decrease the level of parallelism in this RDD.\n",
      " |      Internally, this uses a shuffle to redistribute data.\n",
      " |      If you are decreasing the number of partitions in this RDD, consider\n",
      " |      using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |      >>> len(rdd.repartition(2).glom().collect())\n",
      " |      2\n",
      " |      >>> len(rdd.repartition(10).glom().collect())\n",
      " |      10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f7698dfb320>, ascending=True, keyfunc=<function RDD.<lambda> at 0x7f76807380e0>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, w) in `other`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
      " |      :param fraction: expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      :param seed: seed for the random number generator\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: (None by default)\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is :class:`pyspark.serializers.PickleSerializer`, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      :param path: path to sequence file\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      :param path: path to text file\n",
      " |      :param compressionCodecClass: (None by default) string i.e.\n",
      " |          \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      \n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda> at 0x7f7680738200>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in `self` that is not contained in `other`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in `self` that has no pair with matching\n",
      " |      key in `other`.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
      " |      \n",
      " |      :param prefetchPartitions: If Spark should pre-fetch the next partition\n",
      " |                                 before it is needed.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: It returns the list sorted in descending order.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `blocking` to specify whether to block until all\n",
      " |         blocks are deleted.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      :meth:`zipWithIndex`.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  context\n",
      " |      The :class:`SparkContext` that this RDD was created on.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_is_barrier',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'barrier',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the 100th Etext file presented by Project Gutenberg, and'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.first()  # First Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 100th Etext file presented by Project Gutenberg, and\n",
      "is presented in cooperation with World Library, Inc., from their\n",
      "Library of the Future and Shakespeare CDROMS.  Project Gutenberg\n",
      "often releases Etexts that are NOT placed in the Public Domain!!\n",
      "\n",
      "Shakespeare\n",
      "\n",
      "*This Etext has certain copyright implications you should read!*\n",
      "\n",
      "<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\n"
     ]
    }
   ],
   "source": [
    "for i in text_data.take(10):     # printing first 10 elements\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = sc.textFile('sample-2mb-text-file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Velit scelerisque in dictum non consectetur a erat. Sit amet justo donec enim diam vulputate. Id aliquet lectus proin nibh nisl condimentum id venenatis a. Eget gravida cum sociis natoque penatibus et magnis dis. Habitant morbi tristique senectus et netus et. Interdum consectetur libero id faucibus nisl tincidunt eget nullam. Aliquam purus sit amet luctus. Fringilla ut morbi tincidunt augue interdum velit. Neque sodales ut etiam sit. Quam viverra orci sagittis eu volutpat odio facilisis mauris. Ornare suspendisse sed nisi lacus sed. Iaculis at erat pellentesque adipiscing commodo elit at imperdiet dui. Quam nulla porttitor massa id neque aliquam vestibulum morbi. Dignissim diam quis enim lobortis scelerisque fermentum dui faucibus. Turpis egestas integer eget aliquet.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_map = data2.map(lambda c:c.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'ipsum',\n",
       " 'dolor',\n",
       " 'sit',\n",
       " 'amet,',\n",
       " 'consectetur',\n",
       " 'adipiscing',\n",
       " 'elit,',\n",
       " 'sed',\n",
       " 'do',\n",
       " 'eiusmod',\n",
       " 'tempor',\n",
       " 'incididunt',\n",
       " 'ut',\n",
       " 'labore',\n",
       " 'et',\n",
       " 'dolore',\n",
       " 'magna',\n",
       " 'aliqua.',\n",
       " 'Velit',\n",
       " 'scelerisque',\n",
       " 'in',\n",
       " 'dictum',\n",
       " 'non',\n",
       " 'consectetur',\n",
       " 'a',\n",
       " 'erat.',\n",
       " 'Sit',\n",
       " 'amet',\n",
       " 'justo',\n",
       " 'donec',\n",
       " 'enim',\n",
       " 'diam',\n",
       " 'vulputate.',\n",
       " 'Id',\n",
       " 'aliquet',\n",
       " 'lectus',\n",
       " 'proin',\n",
       " 'nibh',\n",
       " 'nisl',\n",
       " 'condimentum',\n",
       " 'id',\n",
       " 'venenatis',\n",
       " 'a.',\n",
       " 'Eget',\n",
       " 'gravida',\n",
       " 'cum',\n",
       " 'sociis',\n",
       " 'natoque',\n",
       " 'penatibus',\n",
       " 'et',\n",
       " 'magnis',\n",
       " 'dis.',\n",
       " 'Habitant',\n",
       " 'morbi',\n",
       " 'tristique',\n",
       " 'senectus',\n",
       " 'et',\n",
       " 'netus',\n",
       " 'et.',\n",
       " 'Interdum',\n",
       " 'consectetur',\n",
       " 'libero',\n",
       " 'id',\n",
       " 'faucibus',\n",
       " 'nisl',\n",
       " 'tincidunt',\n",
       " 'eget',\n",
       " 'nullam.',\n",
       " 'Aliquam',\n",
       " 'purus',\n",
       " 'sit',\n",
       " 'amet',\n",
       " 'luctus.',\n",
       " 'Fringilla',\n",
       " 'ut',\n",
       " 'morbi',\n",
       " 'tincidunt',\n",
       " 'augue',\n",
       " 'interdum',\n",
       " 'velit.',\n",
       " 'Neque',\n",
       " 'sodales',\n",
       " 'ut',\n",
       " 'etiam',\n",
       " 'sit.',\n",
       " 'Quam',\n",
       " 'viverra',\n",
       " 'orci',\n",
       " 'sagittis',\n",
       " 'eu',\n",
       " 'volutpat',\n",
       " 'odio',\n",
       " 'facilisis',\n",
       " 'mauris.',\n",
       " 'Ornare',\n",
       " 'suspendisse',\n",
       " 'sed',\n",
       " 'nisi',\n",
       " 'lacus',\n",
       " 'sed.',\n",
       " 'Iaculis',\n",
       " 'at',\n",
       " 'erat',\n",
       " 'pellentesque',\n",
       " 'adipiscing',\n",
       " 'commodo',\n",
       " 'elit',\n",
       " 'at',\n",
       " 'imperdiet',\n",
       " 'dui.',\n",
       " 'Quam',\n",
       " 'nulla',\n",
       " 'porttitor',\n",
       " 'massa',\n",
       " 'id',\n",
       " 'neque',\n",
       " 'aliquam',\n",
       " 'vestibulum',\n",
       " 'morbi.',\n",
       " 'Dignissim',\n",
       " 'diam',\n",
       " 'quis',\n",
       " 'enim',\n",
       " 'lobortis',\n",
       " 'scelerisque',\n",
       " 'fermentum',\n",
       " 'dui',\n",
       " 'faucibus.',\n",
       " 'Turpis',\n",
       " 'egestas',\n",
       " 'integer',\n",
       " 'eget',\n",
       " 'aliquet.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_map.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redDict(x:list):\n",
    "    ret_dict={}\n",
    "    for c in x:\n",
    "        if c not in ret_dict():\n",
    "            ret_dict[c]=1\n",
    "        else:\n",
    "            ret_dict[c]+=1\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_reduce = data2_map.reduceByKey(redDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bypass_serializer',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_is_barrier',\n",
       " '_is_pipelinable',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_jrdd_val',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_prev_jrdd',\n",
       " '_prev_jrdd_deserializer',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'barrier',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'func',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_barrier',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'preservesPartitioning',\n",
       " 'prev',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data2_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 7, 10.0.2.15, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in main\n    process()\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 587, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 1933, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in main\n    process()\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 587, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 1933, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6c882f6f8219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata2_reduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m         \"\"\"\n\u001b[0;32m-> 1451\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 7, 10.0.2.15, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in main\n    process()\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 587, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 1933, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in main\n    process()\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 587, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\", line 1933, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/pyspark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data2_reduce.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
